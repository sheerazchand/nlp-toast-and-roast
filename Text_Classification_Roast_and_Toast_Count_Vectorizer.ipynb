{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPrPJuBeU5tl",
    "outputId": "0e0fbf7c-3ed0-4a95-d8c2-9af085dd1c54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install contractions -q\n",
    "import contractions\n",
    "\n",
    "!pip install textacy -q\n",
    "from textacy import preprocessing\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AhK2gu1fxI8"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5BusQWXfwwc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q4VTSH6f2xM"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"/content/drive/My Drive/Colab Notebooks/NLP/toast-roast-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Concatenate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1en-E25Uf5lN",
    "outputId": "3f572132-e2d1-4f91-a58e-23a2b64ba53e"
   },
   "outputs": [],
   "source": [
    "# Load roast and toast files for each split\n",
    "df_train_roast = pd.read_excel(dataset_path / \"Train__Roast.xlsx\", index_col=None)\n",
    "df_train_toast = pd.read_excel(dataset_path / \"Train__Toast.xlsx\", index_col=None)\n",
    "df_val_roast   = pd.read_excel(dataset_path / \"Val__Roast.xlsx\", index_col=None)\n",
    "df_val_toast   = pd.read_excel(dataset_path / \"Val__Toast.xlsx\", index_col=None)\n",
    "df_test_roast  = pd.read_excel(dataset_path / \"Test__Roast.xlsx\", index_col=None)\n",
    "df_test_toast  = pd.read_excel(dataset_path / \"Test__Toast.xlsx\", index_col=None)\n",
    "\n",
    "# Assign labels: Roast = 0, Toast = 1\n",
    "df_train_roast[\"label\"] = 0\n",
    "df_train_toast[\"label\"] = 1\n",
    "df_val_roast[\"label\"]   = 0\n",
    "df_val_toast[\"label\"]   = 1\n",
    "df_test_roast[\"label\"]  = 0\n",
    "df_test_toast[\"label\"]  = 1\n",
    "\n",
    "# Concatenate all splits into one dataframe\n",
    "df = pd.concat([df_train_roast, df_train_toast,\n",
    "                df_val_roast, df_val_toast,\n",
    "                df_test_roast, df_test_toast], ignore_index=True)\n",
    "\n",
    "text_all = df[\"text\"].astype(str).to_list()\n",
    "labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(f\"Total samples: {len(text_all)}, Labels shape: {labels.shape}\")\n",
    "print(f\"Unique labels: {np.unique(labels)}, Roast (0): {sum(labels==0)}, Toast (1): {sum(labels==1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0eejivNgY0v"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl_aTuI3gmEu"
   },
   "outputs": [],
   "source": "# Define function for punctuation removal\ndef f_punctuation_removal(text):\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    return text\n\n# Define function for whitespace normalization\ndef f_whitespace_normalization(text):\n  text = re.sub(r'[\\s]+', ' ', text).strip()\n  return text\n\n# Remove contractions\ntext_all = [contractions.fix( text_current, slang=False) for text_current in text_all]\n\n# Other steps... (feel free to add/remove as per your informed choice)\ntext_data = []\n\nfor text_curr in text_all:\n\n  # - Tokenize\n  tokens = word_tokenize(text_curr)\n\n  # - Lemmatize\n  lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n\n  # - Remove stopwords\n  filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n\n  # - Convert from tokens to sentence, lower case, punctuation removal,and append to list\n  sent = \" \".join( filtered_tokens ).lower()\n  sent = f_punctuation_removal( sent )\n\n  # - Remove hashtags, user handles, emojis, urls, quotation marks, brackets, numbers\n  sent = preprocessing.replace.hashtags( sent, repl=\" \")\n  sent = preprocessing.replace.user_handles( sent, repl=\" \")\n  sent = preprocessing.replace.emojis( sent, repl=\" \")\n  sent = preprocessing.replace.urls( sent, repl=\" \")\n\n  sent = preprocessing.normalize.quotation_marks( sent )\n  sent = preprocessing.remove.html_tags( sent )\n  sent = preprocessing.remove.brackets( sent )\n  sent = re.sub('\"', '', sent)\n  sent = re.sub(\"'\", '', sent)\n  sent = preprocessing.replace.numbers( sent, repl=\" \" )\n\n  # - Normalize whitespace\n  sent = f_whitespace_normalization( sent )\n\n  # - Append to list\n  text_data.append( sent )"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcfSOfnUofwU"
   },
   "source": [
    "## Text Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEoPIpBin-Zm",
    "outputId": "962112a2-dfad-4d24-8560-e1416ae82663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 0.5 200 0.5\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing subsets (80% train, 20% test)\n",
    "text_Train, text_Test, Labs_Train, Labs_Test = train_test_split(text_data, labels, test_size=0.2, stratify=labels, random_state=0)\n",
    "\n",
    "print( len(text_Train), sum(Labs_Train)/len(Labs_Train), len(text_Test), sum(Labs_Test)/len(Labs_Test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uGKRioJDpDpX"
   },
   "outputs": [],
   "source": [
    "# Set custom hyperparameters for CountVectorizer\n",
    "vectorizer = CountVectorizer( ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
    "                              max_features=5000  # Limit the number of features to 5000\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zhMJhxjpSB8",
    "outputId": "443470a5-e116-4e06-c386-36863aae654b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5000) (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Apply CountVectorizer with custom hyperparameters to transform the text data into numerical features\n",
    "Feats_Train_countvec = vectorizer.fit_transform( text_Train ).toarray()\n",
    "Feats_Test_countvec = vectorizer.transform( text_Test ).toarray()\n",
    "\n",
    "print( Feats_Train_countvec.shape, Feats_Test_countvec.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o46EXzoQqBQK",
    "outputId": "aa8872ca-4e86-4f97-c149-8c0b95ab8c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025 0.04993746088859545\n"
     ]
    }
   ],
   "source": [
    "print( np.mean(Feats_Train_countvec[:,0]), np.std(Feats_Train_countvec[:,0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UijU4BELpnBe",
    "outputId": "b81b9594-d429-48ff-8a8f-d0bd88e0888e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5000) (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "Feats_Train_countvec = scaler.fit_transform( Feats_Train_countvec )\n",
    "Feats_Test_countvec = scaler.transform( Feats_Test_countvec )\n",
    "\n",
    "print( Feats_Train_countvec.shape, Feats_Test_countvec.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYS5Ur8rp_zk",
    "outputId": "8dc2bf85-535c-4dac-ed58-85f51184166e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3322676295501878e-17 1.0000000000000016\n"
     ]
    }
   ],
   "source": [
    "print( np.mean(Feats_Train_countvec[:,0]), np.std(Feats_Train_countvec[:,0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "DQ-ssoTtqaDH",
    "outputId": "8483b07a-c6a2-4cf9-c533-2682e5b70ae2"
   },
   "outputs": [],
   "source": [
    "# Train an SVM classifier with the 'linear' kernel\n",
    "clf = LinearSVC( class_weight=\"balanced\", max_iter=1000, random_state=0)\n",
    "clf.fit( Feats_Train_countvec, Labs_Train)\n",
    "\n",
    "# Predict the classes of the testing data\n",
    "predLabs_Test = clf.predict( Feats_Test_countvec )\n",
    "\n",
    "# Evaluate the performance of the model using classification metrics\n",
    "print(classification_report( Labs_Test, predLabs_Test, target_names=[\"Roast\", \"Toast\"] ))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix( Labs_Test, predLabs_Test)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Roast\", \"Toast\"], yticklabels=[\"Roast\", \"Toast\"])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix - Count Vectorizer (Roast vs Toast)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}