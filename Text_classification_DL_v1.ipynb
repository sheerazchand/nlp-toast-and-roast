{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9uXgVu4IoFnA",
        "outputId": "f9491f2c-1559-4cb1-f792-5587c52f5cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-2782079925.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2782079925.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from google.colab import drive\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# Optional: uncomment these lines if you run in Google Colab and store data on Drive.\n",
        " from google.colab import drive\n",
        " drive.mount('/content/drive/My Drive/Colab Notebooks/NLP/toast-roast-dataset')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C_P5L-bgOEU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Bidirectional, LSTM, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import sklearn.metrics\n",
        "from __future__ import print_function\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "oGOXdqi9sLrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph to plot history\n",
        "\n",
        "def f_plot_history( history, file_to_save ):\n",
        "\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(5, 5),  constrained_layout=True)\n",
        "\n",
        "  # plot accuracy subplot\n",
        "  ax = axes[0]\n",
        "  ax.plot(epochs, acc, 'b--', label='Training acc')\n",
        "  ax.plot(epochs, val_acc, 'r-o', label='Validation acc')\n",
        "  ax.set_title('Training and validation accuracy')\n",
        "  ax.set_xlabel('Epochs')\n",
        "  ax.set_ylabel('Accuracy')\n",
        "  ax.legend()\n",
        "  ax.grid()\n",
        "  #ax.set_ylim(0,1)\n",
        "  ax.legend()\n",
        "\n",
        "  # plot loss subplot\n",
        "  ax = axes[1]\n",
        "  ax.plot(epochs, loss, 'b--', label='Training loss')\n",
        "  ax.plot(epochs, val_loss, 'r-o', label='Validation loss')\n",
        "  ax.set_title('Training and validation loss')\n",
        "  ax.set_xlabel('Epochs')\n",
        "  ax.set_ylabel('Loss')\n",
        "  ax.legend()\n",
        "  ax.grid()\n",
        "  ax.legend()\n",
        "\n",
        "  plt.savefig( file_to_save, dpi=400 )\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  return plt\n",
        "\n",
        "\n",
        "def f_create_dir( pathOut ):\n",
        "    if not os.path.exists( pathOut ):\n",
        "        os.makedirs( pathOut )"
      ],
      "metadata": {
        "id": "YHbTDpgSsM9G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "DUMsLEYQg9_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ExpID = \"PPv1\"\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Local dataset folder (update if you place files elsewhere)\n",
        "pathIn_main = Path(\"dataset\")\n",
        "\n",
        "fileIn_Train_Toast = pathIn_main / \"Train__Toast.xlsx\"\n",
        "fileIn_Train_Roast = pathIn_main / \"Train__Roast.xlsx\"\n",
        "\n",
        "fileIn_Val_Toast = pathIn_main / \"Val__Toast.xlsx\"\n",
        "fileIn_Val_Roast = pathIn_main / \"Val__Roast.xlsx\"\n",
        "\n",
        "fileIn_Test_Toast = pathIn_main / \"Test__Toast.xlsx\"\n",
        "fileIn_Test_Roast = pathIn_main / \"Test__Roast.xlsx\"\n",
        "\n",
        "pathSave_main_Results = Path(\"results\") / ExpID\n",
        "f_create_dir(pathSave_main_Results)\n",
        "\n",
        "pathSave_trained_models = Path(\"trained_models\") / ExpID\n",
        "f_create_dir(pathSave_trained_models)\n"
      ],
      "metadata": {
        "id": "6AX7xVeJgUlO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Train_Toast = pd.read_excel( fileIn_Train_Toast, index_col=None )\n",
        "df_Train_Toast.dropna( inplace=True )\n",
        "\n",
        "df_Train_Roast = pd.read_excel( fileIn_Train_Roast, index_col=None )\n",
        "df_Train_Roast.dropna( inplace=True )\n",
        "\n",
        "df_Val_Toast = pd.read_excel( fileIn_Val_Toast, index_col=None )\n",
        "df_Val_Toast.dropna( inplace=True )\n",
        "\n",
        "df_Val_Roast = pd.read_excel( fileIn_Val_Roast, index_col=None )\n",
        "df_Val_Roast.dropna( inplace=True )\n",
        "\n",
        "df_Test_Toast = pd.read_excel( fileIn_Test_Toast, index_col=None )\n",
        "df_Test_Toast.dropna( inplace=True )\n",
        "\n",
        "df_Test_Roast = pd.read_excel( fileIn_Test_Roast, index_col=None )\n",
        "df_Test_Roast.dropna( inplace=True )"
      ],
      "metadata": {
        "id": "eVxz-EHqhgKc",
        "outputId": "eacb2e95-ab46-4236-ae45-111a664d80cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset/Train__Toast.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-889345201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_Train_Toast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfileIn_Train_Toast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_Train_Toast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_Train_Roast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfileIn_Train_Roast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_Train_Roast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/Train__Toast.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_Train = df_Train_Toast[\"text_PP\"].tolist() + df_Train_Roast[\"text_PP\"].tolist()\n",
        "Labs_Train = np.hstack( [np.zeros(df_Train_Toast[\"text_PP\"].shape[0]), np.ones(df_Train_Roast[\"text_PP\"].shape[0])] )\n",
        "\n",
        "text_Val = df_Val_Toast[\"text_PP\"].tolist() + df_Val_Roast[\"text_PP\"].tolist()\n",
        "Labs_Val = np.hstack( [np.zeros(df_Val_Toast[\"text_PP\"].shape[0]), np.ones(df_Val_Roast[\"text_PP\"].shape[0])] )\n",
        "\n",
        "text_Test = df_Test_Toast[\"text_PP\"].tolist() + df_Test_Roast[\"text_PP\"].tolist()\n",
        "Labs_Test = np.hstack( [np.zeros(df_Test_Toast[\"text_PP\"].shape[0]), np.ones(df_Test_Roast[\"text_PP\"].shape[0])] )\n",
        "\n",
        "\n",
        "Labs_Train_cat = to_categorical( Labs_Train, num_classes=2, dtype='float32' )\n",
        "Labs_Val_cat = to_categorical( Labs_Val, num_classes=2, dtype='float32' )\n",
        "Labs_Test_cat = to_categorical( Labs_Test, num_classes=2, dtype='float32' )\n"
      ],
      "metadata": {
        "id": "VoJT4lupiCBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( len(text_Train), len(text_Val), len(text_Test) )\n",
        "\n",
        "print( sum(Labs_Train)/len(text_Train), sum(Labs_Val)/len(text_Val), sum(Labs_Test)/len(text_Test) )"
      ],
      "metadata": {
        "id": "2pHuvvdvqy37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDOfjisq8kc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF + Linear SVM Classifier"
      ],
      "metadata": {
        "id": "ToG5xDaah6bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(lowercase=False, max_features=10000)\n",
        "\n",
        "Feats_Train = vectorizer.fit_transform(text_Train)\n",
        "Feats_Val = vectorizer.transform( text_Val )\n",
        "Feats_Test = vectorizer.transform( text_Test )\n",
        "\n",
        "print( Feats_Train.shape, Feats_Val.shape, Feats_Test.shape )\n",
        "\n",
        "LinearSVC_clf = LinearSVC(C=1e-5, class_weight=\"balanced\", max_iter=1000, random_state=0)\n",
        "\n",
        "LinearSVC_clf.fit(Feats_Train, Labs_Train)\n",
        "\n",
        "predLabs_Val =  LinearSVC_clf.predict(Feats_Val)\n",
        "acc_Val = sklearn.metrics.accuracy_score(Labs_Val, predLabs_Val)\n",
        "\n",
        "predLabs_Test = LinearSVC_clf.predict(Feats_Test)\n",
        "acc_Test = sklearn.metrics.accuracy_score(Labs_Test, predLabs_Test)\n",
        "\n",
        "print( acc_Val, acc_Test )"
      ],
      "metadata": {
        "id": "Dlme7jfEh5bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Labs_Val.shape"
      ],
      "metadata": {
        "id": "x9KuGlS-1PQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Confusion matrix and classifier report for Val\n",
        "cm = confusion_matrix(Labs_Val, predLabs_Val)\n",
        "\n",
        "# Transform to df for easier plotting\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = [\"Toast\", \"Roast\"],\n",
        "                     columns = [\"Toast\", \"Roast\"])\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
        "plt.title('Accuracy: {0:.3f}'.format(accuracy_score(Labs_Val, predLabs_Val)))\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig( pathSave_main_Results+\"/ConfMat_Val_TFIDF__\"+ExpID+\".png\", dpi=400 )\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(classification_report(Labs_Val, predLabs_Val, target_names=[\"Toast\", \"Roast\"]))"
      ],
      "metadata": {
        "id": "CJCgvlyI852W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predLabs_Test.shape"
      ],
      "metadata": {
        "id": "5fgJ0Ybl1W9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Confusion matrix and classifier report for Test\n",
        "cm = confusion_matrix(Labs_Test, predLabs_Test)\n",
        "\n",
        "# Transform to df for easier plotting\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = [\"Toast\", \"Roast\"],\n",
        "                     columns = [\"Toast\", \"Roast\"])\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
        "plt.title('Accuracy: {0:.3f}'.format(accuracy_score(Labs_Test, predLabs_Test)))\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig( pathSave_main_Results+\"/ConfMat_Test_TFIDF__\"+ExpID+\".png\", dpi=400 )\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(classification_report(Labs_Test, predLabs_Test, target_names=[\"Toast\", \"Roast\"]))"
      ],
      "metadata": {
        "id": "-pmGBMWD94wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras Tokenize"
      ],
      "metadata": {
        "id": "pty5DnHVtGR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum number of words to keep in the vocabulary\n",
        "vocabulary_size = 10000\n",
        "\n",
        "# Define the maximum length of the input sequences\n",
        "max_length = 300\n",
        "\n",
        "# Create a tokenizer object and fit it on the training data\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(text_Train)\n",
        "\n",
        "# Convert the text data to sequences of integers\n",
        "sequences_Train = tokenizer.texts_to_sequences(text_Train)\n",
        "sequences_Val = tokenizer.texts_to_sequences(text_Val)\n",
        "sequences_Test = tokenizer.texts_to_sequences(text_Test)\n",
        "\n",
        "\n",
        "\n",
        "# Pad the sequences to a fixed length using the pad_sequences function\n",
        "padded_Train = pad_sequences(sequences_Train, maxlen=max_length)\n",
        "padded_Val = pad_sequences(sequences_Val, maxlen=max_length)\n",
        "padded_Test = pad_sequences(sequences_Test, maxlen=max_length)\n",
        "\n",
        "# Verify the shape of the padded sequences\n",
        "print(\"Shape of padded_Train:\", padded_Train.shape)\n",
        "print(\"Shape of padded_Val:\", padded_Val.shape)\n",
        "print(\"Shape of padded_Test:\", padded_Test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KPzhvW32rX6N",
        "outputId": "ed1f9056-453e-47e7-bb28-55199e13023c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_Train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1846397762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a tokenizer object and fit it on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_Train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Convert the text data to sequences of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_Train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Embedding with Dense"
      ],
      "metadata": {
        "id": "Hb3TaRqdtkEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the size of the vocabulary, which will be used to encode the text\n",
        "\n",
        "modelname = \"KerasEmbDense\"\n",
        "\n",
        "input_shape = (padded_Train.shape[1],)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer to the model, which will learn a dense vector representation of the words\n",
        "# The input_dim parameter should be set to the size of the vocabulary\n",
        "# The output_dim parameter specifies the size of the dense vector representation\n",
        "# The input_length parameter should be set to the maximum length of the input sequences\n",
        "model.add(Embedding(input_dim=vocabulary_size, output_dim=100, input_shape=input_shape ))\n",
        "\n",
        "# Add a Flatten layer to the model, which will flatten the output of the Embedding layer into a 1D vector\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the early stopping and model checkpoint callbacks\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max')\n",
        "model_checkpoint = ModelCheckpoint(pathSave_trained_models+\"/\"+modelname+'_best_model_{val_accuracy:.4f}.h5', monitor='val_accuracy', save_best_only=True, verbose=1, mode='max')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(padded_Train, Labs_Train_cat, epochs=num_epochs, batch_size=32, validation_data=(padded_Val, Labs_Val_cat), callbacks=[early_stop, model_checkpoint])\n",
        "\n",
        "# Load the saved best model\n",
        "model = load_model(pathSave_trained_models+\"/\"+modelname+\"_best_model_\"+\"{:.4f}\".format(model_checkpoint.best)+\".h5\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(padded_Test, Labs_Test_cat)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(padded_Test, Labs_Test_cat)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "gDb6CcLnrug2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_plot_history( history, pathSave_main_Results+\"/DLTrainValCurve_KerasEmbDense__\"+ExpID+\".png\" )"
      ],
      "metadata": {
        "id": "e3PtSXF_LnEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Confusion matrix and classifier report for Val\n",
        "\n",
        "predLabs_Val = np.argmax( model.predict(padded_Val), axis=1)\n",
        "\n",
        "cm = confusion_matrix(Labs_Val, predLabs_Val)\n",
        "\n",
        "# Transform to df for easier plotting\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = [\"Toast\", \"Roast\"],\n",
        "                     columns = [\"Toast\", \"Roast\"])\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
        "plt.title('Accuracy: {0:.3f}'.format(accuracy_score(Labs_Val, predLabs_Val)))\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig( pathSave_main_Results+\"/ConfMat_Val_KerasEmbDense__\"+ExpID+\".png\", dpi=400 )\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(classification_report(Labs_Val, predLabs_Val, target_names=[\"Toast\", \"Roast\"]))"
      ],
      "metadata": {
        "id": "izEAl0zPA-bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}