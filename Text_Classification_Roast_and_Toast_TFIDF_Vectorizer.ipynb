{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI7FHwX4vFHB"
   },
   "source": [
    "zss2024 for 23MSDS-NLP course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPrPJuBeU5tl",
    "outputId": "5a852f77-ac6e-4517-8ea4-47c93e2cf573"
   },
   "outputs": [],
   "source": "import importlib\nif importlib.util.find_spec(\"contractions\") is None:\n    !pip install contractions -q\nif importlib.util.find_spec(\"textacy\") is None:\n    !pip install textacy -q"
  },
  {
   "cell_type": "code",
   "source": "import nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MaxAbsScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport contractions\nfrom textacy import preprocessing\n\nimport re\nimport string",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AhK2gu1fxI8"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5BusQWXfwwc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"/content/drive/My Drive/Colab Notebooks/NLP/toast-roast-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q4VTSH6f2xM"
   },
   "source": [
    "## Read and Concatenate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1en-E25Uf5lN",
    "outputId": "b0ae2266-bbdc-489d-de58-b2f8d93a8654"
   },
   "outputs": [],
   "source": [
    "# Load roast and toast files for each split\n",
    "df_train_roast = pd.read_excel(dataset_path / \"Train__Roast.xlsx\", index_col=None)\n",
    "df_train_toast = pd.read_excel(dataset_path / \"Train__Toast.xlsx\", index_col=None)\n",
    "df_val_roast   = pd.read_excel(dataset_path / \"Val__Roast.xlsx\", index_col=None)\n",
    "df_val_toast   = pd.read_excel(dataset_path / \"Val__Toast.xlsx\", index_col=None)\n",
    "df_test_roast  = pd.read_excel(dataset_path / \"Test__Roast.xlsx\", index_col=None)\n",
    "df_test_toast  = pd.read_excel(dataset_path / \"Test__Toast.xlsx\", index_col=None)\n",
    "\n",
    "# Assign labels: Roast = 0, Toast = 1\n",
    "df_train_roast[\"label\"] = 0\n",
    "df_train_toast[\"label\"] = 1\n",
    "df_val_roast[\"label\"]   = 0\n",
    "df_val_toast[\"label\"]   = 1\n",
    "df_test_roast[\"label\"]  = 0\n",
    "df_test_toast[\"label\"]  = 1\n",
    "\n",
    "# Concatenate all splits into one dataframe\n",
    "df = pd.concat([df_train_roast, df_train_toast,\n",
    "                df_val_roast, df_val_toast,\n",
    "                df_test_roast, df_test_toast], ignore_index=True)\n",
    "\n",
    "text_all = df[\"text\"].astype(str).to_list()\n",
    "labels = df[\"label\"].to_numpy()\n",
    "\n",
    "print(f\"Total samples: {len(text_all)}, Labels shape: {labels.shape}\")\n",
    "print(f\"Unique labels: {np.unique(labels)}, Roast (0): {sum(labels==0)}, Toast (1): {sum(labels==1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0eejivNgY0v"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl_aTuI3gmEu"
   },
   "outputs": [],
   "source": [
    "# Define function for punctuation removal\n",
    "def f_punctuation_removal(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    return text\n",
    "\n",
    "# Define function for whitespace normalization\n",
    "def f_whitespace_normalization(text):\n",
    "  text = re.sub(r'[\\s]+', ' ', text).strip()\n",
    "  return text\n",
    "\n",
    "# Remove contractions\n",
    "text_all = [contractions.fix( text_current, slang=False) for text_current in text_all]\n",
    "\n",
    "# Other steps... (feel free to add/remove as per your informed choice)\n",
    "text_data = []\n",
    "\n",
    "for text_curr in text_all:\n",
    "\n",
    "  # - Tokenize\n",
    "  tokens = word_tokenize(text_curr)\n",
    "\n",
    "  # - Lemmatize\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "  # - Remove stopwords\n",
    "  filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "\n",
    "  # - Convert from tokens to sentence, lower case, punctuation removal,and append to list\n",
    "  sent = \" \".join( filtered_tokens ).lower()\n",
    "  sent = f_punctuation_removal( sent )\n",
    "\n",
    "  # - Remove hashtags, user handles, emojis, urls, quotation marks, brackets, numbers\n",
    "  sent = preprocessing.replace.hashtags( sent, repl=\" \")\n",
    "  sent = preprocessing.replace.user_handles( sent, repl=\" \")\n",
    "  sent = preprocessing.replace.emojis( sent, repl=\" \")\n",
    "  sent = preprocessing.replace.urls( sent, repl=\" \")\n",
    "\n",
    "  sent = preprocessing.normalize.quotation_marks( sent )\n",
    "  sent = preprocessing.remove.html_tags( sent )\n",
    "  sent = preprocessing.remove.brackets( sent )\n",
    "  sent = re.sub('\"', '', sent)\n",
    "  sent = re.sub(\"'\", '', sent)\n",
    "  sent = preprocessing.replace.numbers( sent, repl=\" \" )\n",
    "\n",
    "  # - Normalize whitespace\n",
    "  sent = f_whitespace_normalization( sent )\n",
    "\n",
    "  # - Append to list\n",
    "  text_data.append( sent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcfSOfnUofwU"
   },
   "source": [
    "## Text Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEoPIpBin-Zm",
    "outputId": "39f0ba7c-236a-472f-a21d-29dbf2bfdcbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 0.5 200 0.5\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing subsets (80% train, 20% test)\n",
    "text_Train, text_Test, Labs_Train, Labs_Test = train_test_split(text_data, labels, test_size=0.2, stratify=labels, random_state=0)\n",
    "\n",
    "print( len(text_Train), sum(Labs_Train)/len(Labs_Train), len(text_Test), sum(Labs_Test)/len(Labs_Test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uGKRioJDpDpX"
   },
   "outputs": [],
   "source": [
    "# Set custom hyperparameters for TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer( ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
    "                              max_features=5000  # Limit the number of features to 5000\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zhMJhxjpSB8",
    "outputId": "98c64572-ec25-43a3-ace9-2e9dbe7f3be2"
   },
   "outputs": [],
   "source": "# Apply TfidfVectorizer with custom hyperparameters to transform the text data into numerical features\nFeats_Train_tfidf = vectorizer.fit_transform( text_Train )\nFeats_Test_tfidf = vectorizer.transform( text_Test )\n\nprint( Feats_Train_tfidf.shape, Feats_Test_tfidf.shape )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o46EXzoQqBQK",
    "outputId": "ae19be02-63b6-413e-f322-9abb952ff739"
   },
   "outputs": [],
   "source": "print( Feats_Train_tfidf[:,0].mean(), Feats_Train_tfidf[:,0].toarray().std() )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UijU4BELpnBe",
    "outputId": "b64935db-1212-473f-a4c8-f33de5fbcc78"
   },
   "outputs": [],
   "source": "# Apply MaxAbsScaler (works with sparse matrices, preserves sparsity)\nscaler = MaxAbsScaler()\nFeats_Train_tfidf = scaler.fit_transform( Feats_Train_tfidf )\nFeats_Test_tfidf = scaler.transform( Feats_Test_tfidf )\n\nprint( Feats_Train_tfidf.shape, Feats_Test_tfidf.shape )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYS5Ur8rp_zk",
    "outputId": "9294f9ca-cfed-49c9-bd55-be961accea7d"
   },
   "outputs": [],
   "source": "print( Feats_Train_tfidf[:,0].mean(), Feats_Train_tfidf[:,0].toarray().std() )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "DQ-ssoTtqaDH",
    "outputId": "95516a3b-0d37-4577-a9e4-46911fd497a8"
   },
   "outputs": [],
   "source": "# Train an SVM classifier with the 'linear' kernel\nclf = LinearSVC( class_weight=\"balanced\", max_iter=2000, random_state=0)\nclf.fit( Feats_Train_tfidf, Labs_Train)\n\n# Predict the classes of the testing data\npredLabs_Test = clf.predict( Feats_Test_tfidf )\n\n# Evaluate the performance of the model using classification metrics\nprint(classification_report( Labs_Test, predLabs_Test, target_names=[\"Roast\", \"Toast\"] ))\n\n# Calculate the confusion matrix\ncm = confusion_matrix( Labs_Test, predLabs_Test)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Roast\", \"Toast\"], yticklabels=[\"Roast\", \"Toast\"])\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix - TFIDF Vectorizer (Roast vs Toast)')\nplt.show()"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}