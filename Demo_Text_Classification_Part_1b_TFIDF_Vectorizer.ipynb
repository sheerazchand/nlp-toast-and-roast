{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XPrPJuBeU5tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0596dd1-65b9-4cfd-aeee-5693f68cddb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/345.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m337.9/345.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.6/321.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install contractions -q\n",
        "import contractions\n",
        "\n",
        "!pip install textacy -q\n",
        "from textacy import preprocessing\n",
        "\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "-AhK2gu1fxI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset folder from Drive\n",
        "dataset_dir = '/content/drive/My Drive/Colab Notebooks/NLP/toast-roast-dataset'\n",
        "\n",
        "roast_files = [\n",
        "    'Train__Roast.xlsx',\n",
        "    'Val__Roast.xlsx',\n",
        "    'Test__Roast.xlsx',\n",
        "]\n",
        "toast_files = [\n",
        "    'Train__Toast.xlsx',\n",
        "    'Val__Toast.xlsx',\n",
        "    'Test__Toast.xlsx',\n",
        "]\n"
      ],
      "metadata": {
        "id": "B5BusQWXfwwc",
        "outputId": "ee17d2ee-72cb-4fbc-a4b1-fe2f2b0c32b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read dataset"
      ],
      "metadata": {
        "id": "3Q4VTSH6f2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read roast/toast files and concatenate into a single dataset\n",
        "roast_frames = [pd.read_excel(f\"{dataset_dir}/{name}\", index_col=None) for name in roast_files]\n",
        "toast_frames = [pd.read_excel(f\"{dataset_dir}/{name}\", index_col=None) for name in toast_files]\n",
        "\n",
        "df_roast = pd.concat(roast_frames, ignore_index=True).assign(label=0)\n",
        "df_toast = pd.concat(toast_frames, ignore_index=True).assign(label=1)\n",
        "df = pd.concat([df_roast, df_toast], ignore_index=True)\n",
        "\n",
        "text_all = df[\"text\"].astype(str).to_list()\n",
        "labels = df[\"label\"].to_numpy()\n",
        "\n",
        "print(len(text_all), labels.shape, np.unique(labels), sum(labels==0), sum(labels==1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1en-E25Uf5lN",
        "outputId": "343241e2-8c31-4964-849f-d9cb0d9c95e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119261 (119261,) [0 1] 68159 51102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "u0eejivNgY0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for punctuation removal\n",
        "def f_punctuation_removal(text):\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    return text\n",
        "\n",
        "# Define function for whitespace normalization\n",
        "def f_whitespace_normalization(text):\n",
        "  text = re.sub('[\\s]+', ' ', text).strip()\n",
        "  return text\n",
        "\n",
        "# Remove contractions\n",
        "text_all = [contractions.fix( text_current, slang=False) for text_current in text_all]\n",
        "\n",
        "# Other steps... (feel free to add/remove as per your informed choice)\n",
        "text_data = []\n",
        "\n",
        "for text_curr in text_all:\n",
        "\n",
        "  # - Tokenize\n",
        "  tokens = word_tokenize(text_curr)\n",
        "\n",
        "  # - Lemmatize\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "\n",
        "  # - Remove stopwords\n",
        "  filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
        "\n",
        "  # - Convert from tokens to sentence, lower case, punctuation removal,and append to list\n",
        "  sent = \" \".join( filtered_tokens ).lower()\n",
        "  sent = f_punctuation_removal( sent )\n",
        "\n",
        "  # - Remove hashtags, user handles, emojis, urls, quotation marks, brackets, numbers\n",
        "  sent = preprocessing.replace.hashtags( sent, repl=\" \")\n",
        "  sent = preprocessing.replace.user_handles( sent, repl=\" \")\n",
        "  sent = preprocessing.replace.emojis( sent, repl=\" \")\n",
        "  sent = preprocessing.replace.urls( sent, repl=\" \")\n",
        "\n",
        "  sent = preprocessing.normalize.quotation_marks( sent )\n",
        "  sent = preprocessing.remove.html_tags( sent )\n",
        "  sent = preprocessing.remove.brackets( sent )\n",
        "  sent = re.sub('\"', '', sent)\n",
        "  sent = re.sub(\"'\", '', sent)\n",
        "  sent = preprocessing.replace.numbers( sent, repl=\" \" )\n",
        "\n",
        "  # - Normalize whitespace\n",
        "  sent = f_whitespace_normalization( sent )\n",
        "\n",
        "  # - Append to list\n",
        "  text_data.append( sent )\n",
        "\n"
      ],
      "metadata": {
        "id": "dl_aTuI3gmEu",
        "outputId": "535b8c32-d40b-4198-a517-ebf023d517ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-540440677.py:8: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  text = re.sub('[\\s]+', ' ', text).strip()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification pipeline"
      ],
      "metadata": {
        "id": "PcfSOfnUofwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing subsets (80% train, 20% test)\n",
        "text_Train, text_Test, Labs_Train, Labs_Test = train_test_split(text_data, labels, test_size=0.2, stratify=labels, random_state=0)\n",
        "\n",
        "print( len(text_Train), sum(Labs_Train)/len(Labs_Train), len(text_Test), sum(Labs_Test)/len(Labs_Test) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEoPIpBin-Zm",
        "outputId": "def80fd0-88c9-4b3a-f69b-dca82ecfd666"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95408 0.42848608083179607 23853 0.4284995598037983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set custom hyperparameters for TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer( ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
        "                              max_features=5000  # Limit the number of features to 5000\n",
        "                              )"
      ],
      "metadata": {
        "id": "uGKRioJDpDpX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TfidfVectorizer with custom hyperparameters to transform the text data into numerical features\n",
        "Feats_Train_tfidf = vectorizer.fit_transform( text_Train ).toarray()\n",
        "Feats_Test_tfidf = vectorizer.transform( text_Test ).toarray()\n",
        "\n",
        "print( Feats_Train_tfidf.shape, Feats_Test_tfidf.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zhMJhxjpSB8",
        "outputId": "0a88aa56-864b-4418-bfa1-3a2cae1e2451"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(95408, 5000) (23853, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( np.mean(Feats_Train_tfidf[:,0]), np.std(Feats_Train_tfidf[:,0]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o46EXzoQqBQK",
        "outputId": "2a923225-5e73-4051-c397-f332d31e6bb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00014110529570648448 0.008082978711863135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply standard scaling\n",
        "scaler = StandardScaler()\n",
        "Feats_Train_tfidf = scaler.fit_transform( Feats_Train_tfidf )\n",
        "Feats_Test_tfidf = scaler.transform( Feats_Test_tfidf )\n",
        "\n",
        "print( Feats_Train_tfidf.shape, Feats_Test_tfidf.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UijU4BELpnBe",
        "outputId": "4a85df5d-5fdf-47c3-f6c4-f49609f467dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(95408, 5000) (23853, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( np.mean(Feats_Train_tfidf[:,0]), np.std(Feats_Train_tfidf[:,0]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYS5Ur8rp_zk",
        "outputId": "4489ed19-7e9b-4650-f595-3bc610407177"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.9789650166028016e-19 1.0000000000006601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM classifier with the 'linear' kernel\n",
        "clf = LinearSVC( class_weight=\"balanced\", max_iter=1000, random_state=0)\n",
        "clf.fit( Feats_Train_tfidf, Labs_Train)\n",
        "\n",
        "# Predict the classes of the testing data\n",
        "predLabs_Test = clf.predict( Feats_Test_tfidf )\n",
        "\n",
        "# Evaluate the performance of the model using classification metrics\n",
        "print(classification_report( Labs_Test, predLabs_Test ))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix( Labs_Test, predLabs_Test)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix - TFIDF Vectorizer')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DQ-ssoTtqaDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}