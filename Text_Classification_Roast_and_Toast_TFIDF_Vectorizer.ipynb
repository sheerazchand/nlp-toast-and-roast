{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "zss2024 for 23MSDS-NLP course"
   ],
   "metadata": {
    "id": "UI7FHwX4vFHB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XPrPJuBeU5tl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5a852f77-ac6e-4517-8ea4-47c93e2cf573"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install contractions -q\n",
    "import contractions\n",
    "\n",
    "!pip install textacy -q\n",
    "from textacy import preprocessing\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Mount Google Drive",
   "metadata": {
    "id": "-AhK2gu1fxI8"
   }
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive\ndrive.mount('/content/drive')",
   "metadata": {
    "id": "B5BusQWXfwwc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Settings",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\n\ndataset_path = Path(\"/content/drive/My Drive/Colab Notebooks/NLP/toast-roast-dataset\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Read and Concatenate Dataset",
   "metadata": {
    "id": "3Q4VTSH6f2xM"
   }
  },
  {
   "cell_type": "code",
   "source": "# Load roast and toast files for each split\ndf_train_roast = pd.read_excel(dataset_path / \"Train__Roast.xlsx\", index_col=None)\ndf_train_toast = pd.read_excel(dataset_path / \"Train__Toast.xlsx\", index_col=None)\ndf_val_roast   = pd.read_excel(dataset_path / \"Val__Roast.xlsx\", index_col=None)\ndf_val_toast   = pd.read_excel(dataset_path / \"Val__Toast.xlsx\", index_col=None)\ndf_test_roast  = pd.read_excel(dataset_path / \"Test__Roast.xlsx\", index_col=None)\ndf_test_toast  = pd.read_excel(dataset_path / \"Test__Toast.xlsx\", index_col=None)\n\n# Assign labels: Roast = 0, Toast = 1\ndf_train_roast[\"label\"] = 0\ndf_train_toast[\"label\"] = 1\ndf_val_roast[\"label\"]   = 0\ndf_val_toast[\"label\"]   = 1\ndf_test_roast[\"label\"]  = 0\ndf_test_toast[\"label\"]  = 1\n\n# Concatenate all splits into one dataframe\ndf = pd.concat([df_train_roast, df_train_toast,\n                df_val_roast, df_val_toast,\n                df_test_roast, df_test_toast], ignore_index=True)\n\ntext_all = df[\"text\"].astype(str).to_list()\nlabels = df[\"label\"].to_numpy()\n\nprint(f\"Total samples: {len(text_all)}, Labels shape: {labels.shape}\")\nprint(f\"Unique labels: {np.unique(labels)}, Roast (0): {sum(labels==0)}, Toast (1): {sum(labels==1)}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1en-E25Uf5lN",
    "outputId": "b0ae2266-bbdc-489d-de58-b2f8d93a8654"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "id": "u0eejivNgY0v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define function for punctuation removal\n",
    "def f_punctuation_removal(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    return text\n",
    "\n",
    "# Define function for whitespace normalization\n",
    "def f_whitespace_normalization(text):\n",
    "  text = re.sub('[\\s]+', ' ', text).strip()\n",
    "  return text\n",
    "\n",
    "# Remove contractions\n",
    "text_all = [contractions.fix( text_current, slang=False) for text_current in text_all]\n",
    "\n",
    "# Other steps... (feel free to add/remove as per your informed choice)\n",
    "text_data = []\n",
    "\n",
    "for text_curr in text_all:\n",
    "\n",
    "  # - Tokenize\n",
    "  tokens = word_tokenize(text_curr)\n",
    "\n",
    "  # - Lemmatize\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "  # - Remove stopwords\n",
    "  filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "\n",
    "  # - Convert from tokens to sentence, lower case, punctuation removal,and append to list\n",
    "  sent = \" \".join( filtered_tokens ).lower()\n",
    "  sent = f_punctuation_removal( sent )\n",
    "\n",
    "  # - Remove hashtags, user handles, emojis, urls, quotation marks, brackets, numbers\n",
    "  sent = preprocessing.replace.hashtags( sent, repl=\" \")\n",
    "  sent = preprocessing.replace.user_handles( sent, repl=\" \")\n",
    "  sent = preprocessing.replace.emojis( sent, repl=\" \")\n",
    "  sent = preprocessing.replace.urls( sent, repl=\" \")\n",
    "\n",
    "  sent = preprocessing.normalize.quotation_marks( sent )\n",
    "  sent = preprocessing.remove.html_tags( sent )\n",
    "  sent = preprocessing.remove.brackets( sent )\n",
    "  sent = re.sub('\"', '', sent)\n",
    "  sent = re.sub(\"'\", '', sent)\n",
    "  sent = preprocessing.replace.numbers( sent, repl=\" \" )\n",
    "\n",
    "  # - Normalize whitespace\n",
    "  sent = f_whitespace_normalization( sent )\n",
    "\n",
    "  # - Append to list\n",
    "  text_data.append( sent )\n",
    "\n"
   ],
   "metadata": {
    "id": "dl_aTuI3gmEu"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Classification pipeline"
   ],
   "metadata": {
    "id": "PcfSOfnUofwU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing subsets (80% train, 20% test)\n",
    "text_Train, text_Test, Labs_Train, Labs_Test = train_test_split(text_data, labels, test_size=0.2, stratify=labels, random_state=0)\n",
    "\n",
    "print( len(text_Train), sum(Labs_Train)/len(Labs_Train), len(text_Test), sum(Labs_Test)/len(Labs_Test) )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEoPIpBin-Zm",
    "outputId": "39f0ba7c-236a-472f-a21d-29dbf2bfdcbc"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "800 0.5 200 0.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Set custom hyperparameters for TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer( ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
    "                              max_features=5000  # Limit the number of features to 5000\n",
    "                              )"
   ],
   "metadata": {
    "id": "uGKRioJDpDpX"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply TfidfVectorizer with custom hyperparameters to transform the text data into numerical features\n",
    "Feats_Train_tfidf = vectorizer.fit_transform( text_Train ).toarray()\n",
    "Feats_Test_tfidf = vectorizer.transform( text_Test ).toarray()\n",
    "\n",
    "print( Feats_Train_tfidf.shape, Feats_Test_tfidf.shape )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zhMJhxjpSB8",
    "outputId": "98c64572-ec25-43a3-ace9-2e9dbe7f3be2"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(800, 5000) (200, 5000)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print( np.mean(Feats_Train_tfidf[:,0]), np.std(Feats_Train_tfidf[:,0]) )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o46EXzoQqBQK",
    "outputId": "ae19be02-63b6-413e-f322-9abb952ff739"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.000435204945667871 0.008711653536622278\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "Feats_Train_tfidf = scaler.fit_transform( Feats_Train_tfidf )\n",
    "Feats_Test_tfidf = scaler.transform( Feats_Test_tfidf )\n",
    "\n",
    "print( Feats_Train_tfidf.shape, Feats_Test_tfidf.shape )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UijU4BELpnBe",
    "outputId": "b64935db-1212-473f-a4c8-f33de5fbcc78"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(800, 5000) (200, 5000)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print( np.mean(Feats_Train_tfidf[:,0]), np.std(Feats_Train_tfidf[:,0]) )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYS5Ur8rp_zk",
    "outputId": "9294f9ca-cfed-49c9-bd55-be961accea7d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.2204460492503132e-17 0.9999999999999875\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Train an SVM classifier with the 'linear' kernel\nclf = LinearSVC( class_weight=\"balanced\", max_iter=1000, random_state=0)\nclf.fit( Feats_Train_tfidf, Labs_Train)\n\n# Predict the classes of the testing data\npredLabs_Test = clf.predict( Feats_Test_tfidf )\n\n# Evaluate the performance of the model using classification metrics\nprint(classification_report( Labs_Test, predLabs_Test, target_names=[\"Roast\", \"Toast\"] ))\n\n# Calculate the confusion matrix\ncm = confusion_matrix( Labs_Test, predLabs_Test)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Roast\", \"Toast\"], yticklabels=[\"Roast\", \"Toast\"])\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix - TFIDF Vectorizer (Roast vs Toast)')\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "DQ-ssoTtqaDH",
    "outputId": "95516a3b-0d37-4577-a9e4-46911fd497a8"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}